{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.11",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system(u'ls -lL /tmp/')"
      ],
      "metadata": {
        "id": "4d345373-83a2-4618-9552-6695e9f7b16f",
        "outputId": "2668e4df-2a02-4b75-9a17-dc79caa23676"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "total 0\ndrwxrwx---. 4 1000820000 wscommon 49 Jun 19 03:03 1000820000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter the username for your CP4D cluster\n",
        "username = ''\n",
        "# Retrieve your API Key from your CP4D profile\n",
        "api_key = ''\n",
        "# Enter the url for your CP4D cluster\n",
        "url = ''\n",
        "\n",
        "wml_credentials = {\n",
        "    \"username\": username,\n",
        "    \"apikey\": api_key,\n",
        "    \"url\": url,\n",
        "    \"instance_id\": 'openshift',\n",
        "    \"version\": '5.0'\n",
        "}"
      ],
      "metadata": {
        "id": "0172890e-31ff-4d91-98a9-056d031f49f8"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ibm-watson-machine-learning"
      ],
      "metadata": {
        "id": "9dcd1af9-0ca2-4496-85cb-daa5965f7d60",
        "outputId": "dd06e4e9-8c0a-4f65-f236-47c5f15eb17c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: ibm-watson-machine-learning in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (1.0.357)\nCollecting ibm-watson-machine-learning\n  Downloading ibm_watson_machine_learning-1.0.359-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.31.0)\nRequirement already satisfied: urllib3 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (1.26.18)\nRequirement already satisfied: pandas<2.2.0,>=0.24.2 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.1.4)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2024.2.2)\nRequirement already satisfied: lomond in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (0.3.3)\nRequirement already satisfied: tabulate in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (0.8.10)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (23.2)\nRequirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.13.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-watson-machine-learning) (7.0.1)\nRequirement already satisfied: ibm-cos-sdk-core==2.13.4 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.13.4)\nRequirement already satisfied: ibm-cos-sdk-s3transfer==2.13.4 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.13.4)\nRequirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (1.0.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.13.4->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.8.2)\nRequirement already satisfied: numpy<2,>=1.23.2 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (1.26.4)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from requests->ibm-watson-machine-learning) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from requests->ibm-watson-machine-learning) (3.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from importlib-metadata->ibm-watson-machine-learning) (3.17.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT24.1-Premium/lib/python3.11/site-packages (from lomond->ibm-watson-machine-learning) (1.16.0)\nDownloading ibm_watson_machine_learning-1.0.359-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ibm-watson-machine-learning\n  Attempting uninstall: ibm-watson-machine-learning\n    Found existing installation: ibm_watson_machine_learning 1.0.357\n    Uninstalling ibm_watson_machine_learning-1.0.357:\n      Successfully uninstalled ibm_watson_machine_learning-1.0.357\nSuccessfully installed ibm-watson-machine-learning-1.0.359\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "    Found existing installation: ibm_watson_machine_learning 1.0.357\r\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "    Uninstalling ibm_watson_machine_learning-1.0.357:\r\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "      Successfully uninstalled ibm_watson_machine_learning-1.0.357\r\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Successfully installed ibm-watson-machine-learning-1.0.359\r\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watson_machine_learning import APIClient\n",
        "client = APIClient(wml_credentials)"
      ],
      "metadata": {
        "id": "bc9c67f7-0128-45c8-9ae0-638000081a1d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the space_id from your deployment space\n",
        "space_id = ''\n",
        "client.set.default_space(space_id)"
      ],
      "metadata": {
        "id": "0c00108c-b0df-4148-8d9d-d4dc7ffc8fcf",
        "outputId": "2792fb27-3ec4-4519-81a5-f5f66a2a89fd"
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'SUCCESS'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def Granite3B_fp32_ONNX_Inference():\n",
        "\n",
        "    # ---------------------------------------------------------------- LIBRARY INSTALLATION ----------------------------------------------------------------\n",
        "    # To install some of the required packages\n",
        "    import subprocess\n",
        "    subprocess.run(['conda', 'install', 'onnxruntime', '-c', 'https://ausgsa.ibm.com:7191/gsa/ausgsa/projects/o/open-ce/conda/Open-CE-r1.11/1.11.0/opence-p10/'])\n",
        "    subprocess.run(['pip', 'install', 'optimum'])\n",
        "    subprocess.run(['pip', 'install', 'transformers'])\n",
        "    subprocess.run(['pip', 'install', 'huggingface_hub'])\n",
        "    subprocess.run(['pip', 'install', 'boto3==1.26.90'])\n",
        "    subprocess.run(['pip', 'install', 'botocore==1.26.90'])\n",
        "    subprocess.run(['conda', 'install', 'pyyaml', '-c', 'defaults'])\n",
        "    subprocess.run(['pip', 'install', 'accelerate'])\n",
        "    subprocess.run(['pip', 'install', '-U', 'ibm-watson-machine-learning'])\n",
        "\n",
        "    subprocess.run(['mkdir', '/tmp/granite-3b'])\n",
        "\n",
        "    # ---------------------------------------------------------------- Granite-3B config.json creation ----------------------------------------------------------------\n",
        "    import os\n",
        "    import json\n",
        "\n",
        "    # Define the directory and file path\n",
        "    directory = \"/tmp/granite-3b\"\n",
        "    file_path = os.path.join(directory, \"config.json\")\n",
        "\n",
        "    # Define the JSON content\n",
        "    config_content = {\n",
        "      \"activation_function\": \"gelu\",\n",
        "      \"architectures\": [\n",
        "        \"GPTBigCodeForCausalLM\"\n",
        "      ],\n",
        "      \"attention_softmax_in_fp32\": True,\n",
        "      \"attn_pdrop\": 0.1,\n",
        "      \"bos_token_id\": 0,\n",
        "      \"embd_pdrop\": 0.1,\n",
        "      \"eos_token_id\": 0,\n",
        "      \"initializer_range\": 0.02,\n",
        "      \"layer_norm_epsilon\": 1e-05,\n",
        "      \"model_type\": \"gpt_bigcode\",\n",
        "      \"multi_query\": True,\n",
        "      \"n_embd\": 3072,\n",
        "      \"n_head\": 32,\n",
        "      \"n_inner\": 12288,\n",
        "      \"n_layer\": 32,\n",
        "      \"n_positions\": 2048,\n",
        "      \"pad_token_id\": 0,\n",
        "      \"resid_pdrop\": 0.1,\n",
        "      \"scale_attention_softmax_in_fp32\": True,\n",
        "      \"scale_attn_weights\": True,\n",
        "      \"torch_dtype\": \"float32\",\n",
        "      \"transformers_version\": \"4.31.0\",\n",
        "      \"use_cache\": True,\n",
        "      \"vocab_size\": 50304\n",
        "    }\n",
        "\n",
        "    # Write the JSON content to the file\n",
        "    with open(file_path, \"w\") as json_file:\n",
        "        json.dump(config_content, json_file, indent=2)\n",
        "\n",
        "    print(f\"File {file_path} created successfully.\")\n",
        "\n",
        "    # ---------------------------------------------------------------- Granite-3B fp32 PyTorch download ----------------------------------------------------------------\n",
        "    import boto3\n",
        "    from botocore.client import Config\n",
        "    #from botocore import botocore.utils\n",
        "\n",
        "    s3_config = {\n",
        "      \"s3_user\": \"f25e4fb594da49cf93009caa617dc254\",\n",
        "      \"s3_pass\": \"e43ae432d430d9dd4ea2ebb28f400f6c5d5b6134820720a3\",\n",
        "      \"s3_host\": \"http://s3.us-east.cloud-object-storage.appdomain.cloud\",\n",
        "      \"s3_bucket\": \"inter-ckpts\",\n",
        "    }\n",
        "\n",
        "    s3_client = boto3.session.Session().resource(\n",
        "      service_name=\"s3\",\n",
        "      endpoint_url=s3_config[\"s3_host\"],\n",
        "      aws_access_key_id=s3_config[\"s3_user\"],\n",
        "      aws_secret_access_key=s3_config[\"s3_pass\"],\n",
        "      config=Config(signature_version=\"s3v4\"),\n",
        "    )\n",
        "    bucket = s3_client.Bucket(s3_config[\"s3_bucket\"])\n",
        "\n",
        "    for bucket_object in bucket.objects.all():\n",
        "        print(bucket_object)\n",
        "\n",
        "    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/config.json\",\"/tmp/granite-3b/generation_config.json\" )\n",
        "    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/pytorch_model-00001-of-00002.bin\",\"/tmp/granite-3b/pytorch_model-00001-of-00002.bin\" )\n",
        "    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/pytorch_model-00002-of-00002.bin\",\"/tmp/granite-3b/pytorch_model-00002-of-00002.bin\" )\n",
        "    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/pytorch_model.bin.index.json\",\"/tmp/granite-3b/pytorch_model.bin.index.json\" )\n",
        "    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/special_tokens_map.json\",\"/tmp/granite-3b/special_tokens_map.json\" )\n",
        "    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/tokenizer.json\",\"/tmp/granite-3b/tokenizer.json\" )\n",
        "    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/tokenizer_config.json\",\"/tmp/granite-3b/tokenizer_config.json\" )\n",
        "\n",
        "    # ---------------------------------------------------------------- MODEL CONVERSION TO ONNX ----------------------------------------------------------------\n",
        "    subprocess.run(['optimum-cli', 'export', 'onnx', '--model', '/tmp/granite-3b', '/tmp/granite-3b-onnx', '--task', 'text-generation-with-past'])\n",
        "\n",
        "    # ---------------------------------------------------------------- LIBRARY IMPORTS ----------------------------------------------------------------\n",
        "\n",
        "    from transformers import AutoTokenizer\n",
        "    from optimum.onnxruntime import ORTModelForCausalLM\n",
        "    import time\n",
        "    import torch\n",
        "    import onnxruntime\n",
        "    from onnxruntime import ExecutionMode\n",
        "\n",
        "    # Set the random seed for reproducibility\n",
        "    torch.manual_seed(16)\n",
        "\n",
        "    # Set number of threads\n",
        "    options = onnxruntime.SessionOptions()\n",
        "    options.intra_op_num_threads = 12\n",
        "    options.inter_op_num_threads = 12\n",
        "\n",
        "    def inference(input, model_id=\"/tmp/granite-3b-onnx\"):\n",
        "\n",
        "        # Define the question extraction function\n",
        "        def extract_question(input_data):\n",
        "            return input_data[\"input_data\"][0][\"values\"][0]\n",
        "            #return input_data[\"input_data\"][0][\"values\"][0][0]\n",
        "            #return [item[0] for item in input[\"input_data\"][0][\"values\"]]\n",
        "\n",
        "        # Extract the question from input_data\n",
        "        questions = extract_question(input)\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = ORTModelForCausalLM.from_pretrained(model_id, use_cache=True, use_merged=False, use_io_binding=False, session_options=options)\n",
        "\n",
        "        # Tokenize all questions at once\n",
        "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Generate responses\n",
        "        res = model.generate(**inputs, do_sample=True, min_new_tokens=128, max_new_tokens=128, temperature=0.7, top_k=40, top_p=0.95)\n",
        "\n",
        "        # Decode and print the generated responses\n",
        "        outputs = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "\n",
        "        prediction_response = {\n",
        "            'predictions': [{\n",
        "                             'values': [outputs]\n",
        "                             }]\n",
        "\n",
        "        }\n",
        "        return prediction_response\n",
        "\n",
        "    return inference"
      ],
      "metadata": {
        "id": "2bbed386-c903-4880-8575-af9e57c03e83"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the python fuction in the deployment space with specific software specification\n",
        "pyfunc_swspec_id = client.software_specifications.get_uid_by_name(\"runtime-24.1-py3.11\")\n",
        "\n",
        "meta_data = {\n",
        "    client.repository.FunctionMetaNames.NAME: 'Granite-3B FP32 ONNX',\n",
        "    client.repository.FunctionMetaNames.DESCRIPTION: 'Granite-3B fp32 ONNX Inferencing',\n",
        "    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: pyfunc_swspec_id\n",
        "}\n",
        "\n",
        "function_details = client.repository.store_function(meta_props=meta_data, function=Granite3B_fp32_ONNX_Inference)"
      ],
      "metadata": {
        "id": "03bb54bb-117d-4425-bb43-f7465f3dd3bd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "function_uid = client.repository.get_function_uid(function_details)\n",
        "\n",
        "meta_props = {\n",
        "   client.deployments.ConfigurationMetaNames.NAME: \"Granite-3B FP32 ONNX deployment 12 threads\",\n",
        "   client.deployments.ConfigurationMetaNames.DESCRIPTION: \"GRANITE-3B FP32 ONNX WITH 64 GB MEMORY AND 16 vCPU\",\n",
        "   client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: { 'name': 'XL'},\n",
        "   client.deployments.ConfigurationMetaNames.ONLINE: {   }\n",
        "}\n",
        "\n",
        "deployment_details = client.deployments.create(function_uid, meta_props=meta_props)\n",
        "\n",
        "print(deployment_details)\n",
        "\n",
        "deployment_id = client.deployments.get_uid(deployment_details)"
      ],
      "metadata": {
        "id": "11650014-5eb0-4458-9233-4f297293b631"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the deployment ids of the deployments\n",
        "client.deployments.list(limit=100)"
      ],
      "metadata": {
        "id": "d854cb1f-1cc2-4c8b-80fa-e1ac1a2741d7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter deployment ID\n",
        "deployment_id = \"\"\n",
        "\n",
        "# You can scale the deployment to your preferred number of replicas using the code below\n",
        "change_meta = {\n",
        "                client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: {\n",
        "                                       \"name\":\"XL\",\n",
        "                                       \"num_nodes\":8}\n",
        "            }\n",
        "client.deployments.update(deployment_id, change_meta)"
      ],
      "metadata": {
        "id": "9c13c15f-4fe4-4ed8-aa5d-b7d0555cccae"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "16a8bd52-b015-4982-876b-f39fcbc1b42f"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}