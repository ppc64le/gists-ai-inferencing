{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.5", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "get_ipython().system(u'ls -lL /tmp/')", "metadata": {"id": "5fc9e042-8a39-4c82-a2aa-31ae0b3443bc"}, "outputs": [{"name": "stdout", "text": "total 0\ndrwxrwx---. 4 1000840000 wscommon 49 Jun 26 16:32 1000840000\n", "output_type": "stream"}], "execution_count": 1}, {"cell_type": "code", "source": "username = 'cpadmin'\napi_key = 'iwaj8savPEZitY6vXJOjpgq5QlY6RF0PJpjEeLAA'\nurl = 'https://cpd-cpd-instance.apps.ai.toropsp.com/'\n\nwml_credentials = {\n    \"username\": username,\n    \"apikey\": api_key,\n    \"url\": url,\n    \"instance_id\": 'openshift',\n    \"version\": '5.0'\n}", "metadata": {"id": "2bd6e9da-c8ac-4df2-a762-7e703f00443f"}, "outputs": [], "execution_count": 2}, {"cell_type": "code", "source": "!pip install -U ibm-watson-machine-learning ", "metadata": {"id": "f22340b8-d6f0-4ec0-beff-d217265241bd"}, "outputs": [{"name": "stdout", "text": "Requirement already satisfied: ibm-watson-machine-learning in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (1.0.357)\nCollecting ibm-watson-machine-learning\n  Downloading ibm_watson_machine_learning-1.0.359-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.31.0)\nRequirement already satisfied: urllib3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (1.26.18)\nRequirement already satisfied: pandas<2.2.0,>=0.24.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.1.1)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2024.2.2)\nRequirement already satisfied: lomond in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (0.3.3)\nRequirement already satisfied: tabulate in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (0.8.10)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (23.1)\nRequirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (2.13.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-watson-machine-learning) (7.0.1)\nRequirement already satisfied: ibm-cos-sdk-core==2.13.4 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.13.4)\nRequirement already satisfied: ibm-cos-sdk-s3transfer==2.13.4 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.13.4)\nRequirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (0.10.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.13.4->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.8.2)\nRequirement already satisfied: numpy>=1.23.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (1.26.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->ibm-watson-machine-learning) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests->ibm-watson-machine-learning) (3.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from importlib-metadata->ibm-watson-machine-learning) (3.11.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from lomond->ibm-watson-machine-learning) (1.16.0)\nDownloading ibm_watson_machine_learning-1.0.359-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ibm-watson-machine-learning\n  Attempting uninstall: ibm-watson-machine-learning\n    Found existing installation: ibm_watson_machine_learning 1.0.357\n    Uninstalling ibm_watson_machine_learning-1.0.357:\n      Successfully uninstalled ibm_watson_machine_learning-1.0.357\nSuccessfully installed ibm-watson-machine-learning-1.0.359\n", "output_type": "stream"}], "execution_count": 3}, {"cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient \nclient = APIClient(wml_credentials) \nspace_id ='274032d5-1ec4-48b8-b6de-77da8ac0f17c' \nclient.set.default_space(space_id) ", "metadata": {"id": "1f9447b9-6577-45b4-97bd-a04edca9426f"}, "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "'SUCCESS'"}, "metadata": {}}], "execution_count": 4}, {"cell_type": "code", "source": "def Granite3B_fp32_ONNX_Inference():\n\n    # ---------------------------------------------------------------- LIBRARY INSTALLATION ----------------------------------------------------------------\n    # To install some of the required packages \n    import subprocess  \n    subprocess.run(['conda', 'install', 'onnxruntime', '-c', 'https://ausgsa.ibm.com:7191/gsa/ausgsa/projects/o/open-ce/conda/Open-CE-r1.11/1.11.0/opence-p10/']) \n    subprocess.run(['pip', 'install', 'optimum']) \n    subprocess.run(['pip', 'install', 'transformers']) \n    subprocess.run(['pip', 'install', 'huggingface_hub'])\n    subprocess.run(['pip', 'install', 'boto3==1.26.90'])\n    subprocess.run(['pip', 'install', 'botocore==1.26.90'])\n    subprocess.run(['conda', 'install', 'pyyaml', '-c', 'defaults'])\n    subprocess.run(['pip', 'install', 'accelerate'])\n    subprocess.run(['pip', 'install', '-U', 'ibm-watson-machine-learning']) \n    \n    # ---------------------------------------------------------------- API CLIENT ----------------------------------------------------------------\n    # In order to use the Cloud Pak for Data (CP4D) API service inside the deployment, you need to create an API client.\n    username = 'cpadmin'\n    api_key = 'iwaj8savPEZitY6vXJOjpgq5QlY6RF0PJpjEeLAA'\n    url = 'https://cpd-cpd-instance.apps.ai.toropsp.com/'\n    \n    wml_credentials = {\n        \"username\": username,\n        \"apikey\": api_key,\n        \"url\": url,\n        \"instance_id\": 'openshift',\n        \"version\": '5.0'\n    }\n\n    from ibm_watson_machine_learning import APIClient \n    client = APIClient(wml_credentials) \n    space_id ='274032d5-1ec4-48b8-b6de-77da8ac0f17c' \n    client.set.default_space(space_id) \n    \n    subprocess.run(['mkdir', '/tmp/granite-3b']) \n\n    # ---------------------------------------------------------------- Granite-3B config.json creation ----------------------------------------------------------------\n    import os\n    import json\n    \n    # Define the directory and file path\n    directory = \"/tmp/granite-3b\"\n    file_path = os.path.join(directory, \"config.json\")\n    \n    # Define the JSON content\n    config_content = {\n      \"activation_function\": \"gelu\",\n      \"architectures\": [\n        \"GPTBigCodeForCausalLM\"\n      ],\n      \"attention_softmax_in_fp32\": True,\n      \"attn_pdrop\": 0.1,\n      \"bos_token_id\": 0,\n      \"embd_pdrop\": 0.1,\n      \"eos_token_id\": 0,\n      \"initializer_range\": 0.02,\n      \"layer_norm_epsilon\": 1e-05,\n      \"model_type\": \"gpt_bigcode\",\n      \"multi_query\": True,\n      \"n_embd\": 3072,\n      \"n_head\": 32,\n      \"n_inner\": 12288,\n      \"n_layer\": 32,\n      \"n_positions\": 2048,\n      \"pad_token_id\": 0,\n      \"resid_pdrop\": 0.1,\n      \"scale_attention_softmax_in_fp32\": True,\n      \"scale_attn_weights\": True,\n      \"torch_dtype\": \"float32\",\n      \"transformers_version\": \"4.31.0\",\n      \"use_cache\": True,\n      \"vocab_size\": 50304\n    }\n    \n    # Write the JSON content to the file\n    with open(file_path, \"w\") as json_file:\n        json.dump(config_content, json_file, indent=2)\n    \n    print(f\"File {file_path} created successfully.\")\n\n    # ---------------------------------------------------------------- Granite-3B fp32 PyTorch download ----------------------------------------------------------------\n    import boto3\n    from botocore.client import Config\n    #from botocore import botocore.utils\n    \n    s3_config = {\n      \"s3_user\": \"f25e4fb594da49cf93009caa617dc254\",\n      \"s3_pass\": \"e43ae432d430d9dd4ea2ebb28f400f6c5d5b6134820720a3\",\n      \"s3_host\": \"http://s3.us-east.cloud-object-storage.appdomain.cloud\",\n      \"s3_bucket\": \"inter-ckpts\",\n    }\n    \n    s3_client = boto3.session.Session().resource(\n      service_name=\"s3\",\n      endpoint_url=s3_config[\"s3_host\"],\n      aws_access_key_id=s3_config[\"s3_user\"],\n      aws_secret_access_key=s3_config[\"s3_pass\"],\n      config=Config(signature_version=\"s3v4\"),\n    )\n    bucket = s3_client.Bucket(s3_config[\"s3_bucket\"])\n    \n    for bucket_object in bucket.objects.all():\n        print(bucket_object)\n    \n    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/config.json\",\"/tmp/granite-3b/generation_config.json\" )\n    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/pytorch_model-00001-of-00002.bin\",\"/tmp/granite-3b/pytorch_model-00001-of-00002.bin\" )\n    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/pytorch_model-00002-of-00002.bin\",\"/tmp/granite-3b/pytorch_model-00002-of-00002.bin\" )\n    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/pytorch_model.bin.index.json\",\"/tmp/granite-3b/pytorch_model.bin.index.json\" )\n    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/special_tokens_map.json\",\"/tmp/granite-3b/special_tokens_map.json\" )\n    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/tokenizer.json\",\"/tmp/granite-3b/tokenizer.json\" )\n    bucket.download_file(\"granite-3b-base-v2/step_75000_ckpt/tokenizer_config.json\",\"/tmp/granite-3b/tokenizer_config.json\" )\n\n    # ---------------------------------------------------------------- MODEL RETRIEVAL ----------------------------------------------------------------\n    subprocess.run(['optimum-cli', 'export', 'onnx', '--model', '/tmp/granite-3b', '/tmp/granite-3b-onnx', '--task', 'text-generation-with-past'])\n\n    # ---------------------------------------------------------------- LIBRARY IMPORTS ----------------------------------------------------------------\n    \n    from transformers import AutoTokenizer\n    from optimum.onnxruntime import ORTModelForCausalLM\n    import time\n    import torch\n    import onnxruntime\n    from onnxruntime import ExecutionMode\n\n    def inference(input, model_id=\"/tmp/granite-3b-onnx\"):\n\n        # Define the question extraction function\n        def extract_question(input_data):\n            return input_data[\"input_data\"][0][\"values\"][0]\n            #return input_data[\"input_data\"][0][\"values\"][0][0]\n            #return [item[0] for item in input[\"input_data\"][0][\"values\"]]\n        \n        # Extract the question from input_data\n        questions = extract_question(input)\n        \n        # Set the random seed for reproducibility\n        torch.manual_seed(16)\n\n        # Set number of threads\n        options = onnxruntime.SessionOptions()\n        options.intra_op_num_threads = 12\n        options.inter_op_num_threads = 12\n    \n        tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n        tokenizer.pad_token = tokenizer.eos_token\n    \n        model = ORTModelForCausalLM.from_pretrained(model_id, use_cache=True, use_merged=False, use_io_binding=False, session_options=options)\n    \n        # Tokenize questions and calculate the total number of tokens\n        question_tokens = sum(len(tokenizer.encode(question)) for question in questions)\n    \n        # Tokenize all questions at once\n        inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True)\n    \n        # Generate responses\n        res = model.generate(**inputs, do_sample=True, min_new_tokens=128, max_new_tokens=128, temperature=0.7, top_k=40, top_p=0.95)\n    \n        # Decode and print the generated responses\n        outputs = tokenizer.batch_decode(res, skip_special_tokens=True)\n        \"\"\"\n        for i, out in enumerate(outputs):\n            print(f\"Result for Question {i + 1}: {out}\")\n            print(len(tokenizer.encode(out)))\n        \n        # Calculate total tokens in the generated responses\n        total_tokens = sum(len(tokenizer.encode(out)) for out in outputs)\n        # Subtract the question tokens from total tokens to get tokens generated for answers\n        total_tokens -= question_tokens\n        \"\"\"\n        prediction_response = { \n            'predictions': [{\n                             'values': [outputs] \n                             }] \n\n        }\n        return prediction_response\n    \n    return inference", "metadata": {"id": "18b63bf9-c0f0-408a-b19d-497badb482b3"}, "outputs": [], "execution_count": 5}, {"cell_type": "code", "source": "# Creating the python fuction in the deployment space with specific software specification \npyfunc_swspec_id = client.software_specifications.get_uid_by_name(\"runtime-24.1-py3.11\") \n\nmeta_data = { \n    client.repository.FunctionMetaNames.NAME: 'Granite-3B FP32 ONNX', \n    client.repository.FunctionMetaNames.DESCRIPTION: 'Granite-3B fp32 ONNX Inferencing', \n    client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: pyfunc_swspec_id \n} \n\nfunction_details = client.repository.store_function(meta_props=meta_data, function=Granite3B_fp32_ONNX_Inference) ", "metadata": {"id": "75049092-d001-4a86-bbb6-c2c27d87e0b0"}, "outputs": [], "execution_count": 6}, {"cell_type": "code", "source": "function_uid = client.repository.get_function_uid(function_details) \n\nmeta_props = { \n   client.deployments.ConfigurationMetaNames.NAME: \"Granite-3B FP32 ONNX deployment 12 threads dp6\", \n   client.deployments.ConfigurationMetaNames.DESCRIPTION: \"GRANITE-3B FP32 ONNX WITH 64 GB MEMORY AND 16 vCPU\", \n   client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: { 'name': 'XL'}, \n   client.deployments.ConfigurationMetaNames.ONLINE: {   } \n} \n\ndeployment_details = client.deployments.create(function_uid, meta_props=meta_props) \n\nprint(deployment_details) \n\ndeployment_id = client.deployments.get_uid(deployment_details) ", "metadata": {"id": "8099c922-a374-4b9a-af82-63683b2b5aea"}, "outputs": [{"name": "stdout", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '9f6aa966-34d3-4218-9fc2-0f452bc7ea3d' started\n\n#######################################################################################\n\n\ninitializing\nNote: online_url is deprecated and will be removed in a future release. Use serving_urls instead.\n..................................................................\nready\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='cfc57af1-0d15-4129-bb0e-5376590ff733'\n------------------------------------------------------------------------------------------------\n\n\n{'entity': {'asset': {'id': '9f6aa966-34d3-4218-9fc2-0f452bc7ea3d'}, 'custom': {}, 'deployed_asset_type': 'function', 'description': 'GRANITE-3B FP32 ONNX WITH 64 GB MEMORY AND 16 vCPU', 'hardware_spec': {'id': 'd0aa1ae8-a889-42e2-a099-041b604b9289', 'name': 'XL', 'num_nodes': 1}, 'name': 'Granite-3B FP32 ONNX deployment 12 threads dp6', 'online': {}, 'space_id': '274032d5-1ec4-48b8-b6de-77da8ac0f17c', 'status': {'inference': [{'url': 'https://cpd-cpd-instance.apps.ai.toropsp.com/ml/v4/deployments/cfc57af1-0d15-4129-bb0e-5376590ff733/predictions'}], 'online_url': {'url': 'https://cpd-cpd-instance.apps.ai.toropsp.com/ml/v4/deployments/cfc57af1-0d15-4129-bb0e-5376590ff733/predictions'}, 'serving_urls': ['https://cpd-cpd-instance.apps.ai.toropsp.com/ml/v4/deployments/cfc57af1-0d15-4129-bb0e-5376590ff733/predictions'], 'state': 'ready'}}, 'metadata': {'created_at': '2024-06-26T17:09:21.762Z', 'description': 'GRANITE-3B FP32 ONNX WITH 64 GB MEMORY AND 16 vCPU', 'id': 'cfc57af1-0d15-4129-bb0e-5376590ff733', 'modified_at': '2024-06-26T17:09:21.762Z', 'name': 'Granite-3B FP32 ONNX deployment 12 threads dp6', 'owner': '1000331001', 'space_id': '274032d5-1ec4-48b8-b6de-77da8ac0f17c'}, 'system': {'warnings': [{'id': 'Deprecated', 'message': 'online_url is deprecated and will be removed in a future release. Use serving_urls instead.'}]}}\n", "output_type": "stream"}], "execution_count": 12}, {"cell_type": "code", "source": "", "metadata": {"id": "210ddb80-f30d-4985-b69c-53d2f3e3db1b"}, "outputs": [], "execution_count": null}]}